{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import numpy.ma as ma\n",
    "from scipy.stats import mode\n",
    "from scipy.io import loadmat\n",
    "import csv, time, math\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def load_data(filename):\n",
    "    data_dict = loadmat(filename, mat_dtype=True)\n",
    "    return data_dict['train']\n",
    "\n",
    "def write_prediction(label_test, filename):\n",
    "    print \"Writing to file {}...\".format(filename)\n",
    "    label_test = label_test.flatten()\n",
    "    with open(filename, 'wb') as csvfile:\n",
    "        spamwriter = csv.writer(csvfile, delimiter=' ', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "        spamwriter.writerow([\"Id,Category\"])\n",
    "        for i, cat in enumerate(label_test):\n",
    "            spamwriter.writerow([str(i+1) + \",\" + str(int(cat))])\n",
    "\n",
    "def naive_svd(U, S, V, data, filename):\n",
    "    valid, labels = [], []\n",
    "    R_estimate = U.dot(S).dot(V.T)\n",
    "    valid_data = np.zeros((100, 100))\n",
    "    with open(filename) as fh:\n",
    "        for line in fh:\n",
    "            user, joke, score = np.array(line.split(','), dtype=int)\n",
    "            valid_data[user-1][joke-1] = score\n",
    "            valid.append(score)\n",
    "            labels.append(1 if R_estimate[user-1][joke-1]>0 else 0)\n",
    "    return valid, labels\n",
    "\n",
    "def predict_report(R_estimate, filename):\n",
    "    valid, labels = [], []\n",
    "    valid_data = np.zeros((100, 100))\n",
    "    with open(filename) as fh:\n",
    "        for line in fh:\n",
    "            user, joke, score = np.array(line.split(','), dtype=int)\n",
    "            valid_data[user-1][joke-1] = score\n",
    "            valid.append(score)\n",
    "            labels.append(1 if R_estimate[user-1][joke-1]>0 else 0)\n",
    "    return valid, labels\n",
    "\n",
    "def MSE(Up, Vp, data, d, max_iter=1000, reg=10):\n",
    "    Up_old, Vp_old = Up.copy(), Vp.copy()\n",
    "    valid, labels = [], []\n",
    "    for _ in range(max_iter):\n",
    "        for i in range(Up.shape[0]):\n",
    "            A = reg*np.eye(d)\n",
    "            B = np.zeros((d, ))\n",
    "            for j in range(Vp.shape[0]):\n",
    "                if not np.isnan(data[i][j]):\n",
    "                    A += np.outer(Vp[j], Vp[j])\n",
    "                    B += data[i][j]*Vp[j]\n",
    "            u = sp.linalg.solve(A, B)\n",
    "            Up[i] = u\n",
    "\n",
    "        for j in range(Vp.shape[0]):\n",
    "            A = reg*np.eye(d)\n",
    "            B = np.zeros((d, ))\n",
    "            for i in range(Up.shape[0]):\n",
    "                if not np.isnan(data[i][j]):\n",
    "                    A += np.outer(Up[i], Up[i])\n",
    "                    B += data[i][j]*Up[i]\n",
    "            v = sp.linalg.solve(A, B)\n",
    "            Vp[j] = v\n",
    "\n",
    "        if np.allclose(Up_old, Up, atol=1e-08) and np.allclose(Vp_old, Vp, atol=1e-08):\n",
    "            break\n",
    "        else:\n",
    "            Up_old, Vp_old = Up.copy(), Vp.copy()\n",
    "    R_estimate = Up.dot(Vp.T)\n",
    "    return R_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data = load_data(\"./joke_data/joke_train.mat\")\n",
    "data = np.nan_to_num(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U, S, V = np.linalg.svd(data)\n",
    "U2, S2, V2 = U[:, :2], np.diag(S[:2]), V[:2, :].T\n",
    "U5, S5, V5 = U[:, :5], np.diag(S[:5]), V[:5, :].T\n",
    "U10, S10, V10 = U[:, :10], np.diag(S[:10]), V[:10, :].T\n",
    "U20, S20, V20 = U[:, :20], np.diag(S[:20]), V[:20, :].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.705149051491..\n"
     ]
    }
   ],
   "source": [
    "valid, labels = naive_svd(U2, S2, V2, data, \"./joke_data/validation.txt\")\n",
    "print \"Accuracy Score: {}..\".format(metrics.accuracy_score(labels, valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.715447154472..\n"
     ]
    }
   ],
   "source": [
    "valid, labels = naive_svd(U5, S5, V5, data, \"./joke_data/validation.txt\")\n",
    "print \"Accuracy Score: {}..\".format(metrics.accuracy_score(labels, valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.716531165312..\n"
     ]
    }
   ],
   "source": [
    "valid, labels = naive_svd(U10, S10, V10, data, \"./joke_data/validation.txt\")\n",
    "print \"Accuracy Score: {}..\".format(metrics.accuracy_score(labels, valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.685907859079..\n"
     ]
    }
   ],
   "source": [
    "valid, labels = naive_svd(U20, S20, V20, data, \"./joke_data/validation.txt\")\n",
    "print \"Accuracy Score: {}..\".format(metrics.accuracy_score(labels, valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.706775067751..\n"
     ]
    }
   ],
   "source": [
    "R_estimate = MSE(U2, V2, raw_data, 2, max_iter=10, reg=300)\n",
    "valid, labels = predict_report(R_estimate, \"./joke_data/validation.txt\")\n",
    "print \"Accuracy Score: {}..\".format(metrics.accuracy_score(labels, valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.719783197832..\n"
     ]
    }
   ],
   "source": [
    "R_estimate = MSE(U5, V5, raw_data, 5, max_iter=10, reg=300)\n",
    "valid, labels = predict_report(R_estimate, \"./joke_data/validation.txt\")\n",
    "print \"Accuracy Score: {}..\".format(metrics.accuracy_score(labels, valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.731978319783..\n"
     ]
    }
   ],
   "source": [
    "R_estimate = MSE(U10, V10, raw_data, 10, max_iter=10, reg=300)\n",
    "valid, labels = predict_report(R_estimate, \"./joke_data/validation.txt\")\n",
    "print \"Accuracy Score: {}..\".format(metrics.accuracy_score(labels, valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.733604336043..\n"
     ]
    }
   ],
   "source": [
    "R_estimate = MSE(U20, V20, raw_data, 20, max_iter=10, reg=300)\n",
    "valid, labels = predict_report(R_estimate, \"./joke_data/validation.txt\")\n",
    "print \"Accuracy Score: {}..\".format(metrics.accuracy_score(labels, valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.733875338753..\n"
     ]
    }
   ],
   "source": [
    "R_estimate = MSE(U10, V10, raw_data, 10, max_iter=10, reg=300)\n",
    "valid, labels = predict_report(R_estimate, \"./joke_data/validation.txt\")\n",
    "print \"Accuracy Score: {}..\".format(metrics.accuracy_score(labels, valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to file kaggle_submission.txt...\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "with open(\"./joke_data/query.txt\") as fh:\n",
    "    for line in fh:\n",
    "        id, user, joke = np.array(line.split(','), dtype=int)\n",
    "        labels.append(1 if R_estimate[user-1][joke-1]>0 else 0)\n",
    "write_prediction(np.array(labels), \"kaggle_submission.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
